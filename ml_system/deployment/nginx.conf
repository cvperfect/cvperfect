# CVPerfect ML System - Nginx Load Balancer Configuration
# Production-ready reverse proxy with GPU/CPU failover

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    # Basic settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 50M;  # For CV file uploads

    # Logging
    log_format detailed '$remote_addr - $remote_user [$time_local] '
                       '"$request" $status $body_bytes_sent '
                       '"$http_referer" "$http_user_agent" '
                       'rt=$request_time uct="$upstream_connect_time" '
                       'uht="$upstream_header_time" urt="$upstream_response_time" '
                       'upstream="$upstream_addr"';

    access_log /var/log/nginx/access.log detailed;
    error_log /var/log/nginx/error.log warn;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 10240;
    gzip_proxied expired no-cache no-store private must-revalidate;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=ml_limit:10m rate=2r/s;

    # Upstream servers - ML Inference
    upstream ml_inference_servers {
        # Primary: GPU-enabled server
        server ml-inference-gpu:8001 max_fails=2 fail_timeout=30s weight=3;
        
        # Fallback: CPU server (backup)
        server ml-inference-cpu:8001 max_fails=3 fail_timeout=60s weight=1 backup;
        
        # Health checks
        keepalive 32;
        keepalive_requests 100;
        keepalive_timeout 60s;
    }

    # Health check upstream
    upstream health_check {
        server ml-inference-gpu:8001;
        server ml-inference-cpu:8001 backup;
    }

    # Main server configuration
    server {
        listen 80;
        server_name localhost;
        
        # Security headers
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header Referrer-Policy "strict-origin-when-cross-origin" always;
        add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline';" always;

        # Remove server signature
        server_tokens off;

        # Health check endpoint (bypass rate limiting)
        location /health {
            proxy_pass http://health_check/health;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Quick health checks
            proxy_connect_timeout 5s;
            proxy_send_timeout 5s;
            proxy_read_timeout 5s;
            
            access_log off;
        }

        # ML optimization endpoint (rate limited)
        location /optimize {
            # Rate limiting for ML endpoints
            limit_req zone=ml_limit burst=5 nodelay;
            limit_req_status 429;

            # Proxy to ML inference servers
            proxy_pass http://ml_inference_servers/optimize;
            
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeout settings for ML inference
            proxy_connect_timeout 10s;
            proxy_send_timeout 30s;
            proxy_read_timeout 120s;  # Allow for ML processing time
            
            # Buffer settings for large responses
            proxy_buffering on;
            proxy_buffer_size 128k;
            proxy_buffers 4 256k;
            proxy_busy_buffers_size 256k;
            
            # Retry logic
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
            proxy_next_upstream_tries 2;
            proxy_next_upstream_timeout 60s;
        }

        # Batch processing endpoint (lower rate limit)
        location /batch-optimize {
            limit_req zone=ml_limit burst=2 nodelay;
            limit_req_status 429;

            proxy_pass http://ml_inference_servers/batch-optimize;
            
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Extended timeouts for batch processing
            proxy_connect_timeout 10s;
            proxy_send_timeout 60s;
            proxy_read_timeout 300s;  # 5 minutes for batch
            
            proxy_buffering on;
            proxy_buffer_size 256k;
            proxy_buffers 8 256k;
            proxy_busy_buffers_size 512k;
            
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
            proxy_next_upstream_tries 2;
            proxy_next_upstream_timeout 120s;
        }

        # Performance metrics endpoint
        location /performance {
            limit_req zone=api_limit burst=10 nodelay;
            
            proxy_pass http://ml_inference_servers/performance;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            proxy_connect_timeout 5s;
            proxy_send_timeout 10s;
            proxy_read_timeout 10s;
        }

        # Deny access to sensitive files
        location ~ /\. {
            deny all;
            access_log off;
            log_not_found off;
        }

        location ~ /(requirements|Dockerfile|docker-compose) {
            deny all;
            access_log off;
            log_not_found off;
        }

        # Default error pages
        error_page 404 /404.html;
        error_page 500 502 503 504 /50x.html;
        
        location = /50x.html {
            root /usr/share/nginx/html;
        }

        # Custom error page for rate limiting
        error_page 429 @rate_limit_error;
        location @rate_limit_error {
            return 429 '{"error": "Rate limit exceeded", "retry_after": 60}';
            add_header Content-Type application/json always;
        }
    }

    # Status server for monitoring
    server {
        listen 8080;
        server_name localhost;
        
        location /nginx_status {
            stub_status on;
            access_log off;
            allow 127.0.0.1;
            allow 10.0.0.0/8;
            deny all;
        }
        
        location /health {
            return 200 '{"status": "healthy", "service": "nginx"}';
            add_header Content-Type application/json;
        }
    }
}

# Stream configuration for TCP load balancing (if needed)
stream {
    # Upstream for raw TCP connections
    upstream ml_tcp_servers {
        server ml-inference-gpu:8001 max_fails=2 fail_timeout=30s;
        server ml-inference-cpu:8001 max_fails=3 fail_timeout=60s backup;
    }
    
    # TCP proxy (if direct TCP access needed)
    server {
        listen 9001;
        proxy_pass ml_tcp_servers;
        proxy_timeout 120s;
        proxy_connect_timeout 10s;
    }
}